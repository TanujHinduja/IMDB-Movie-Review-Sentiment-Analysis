{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Business\\391_Simple_rnn_imdb Project\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentences\n",
    "sent=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "    'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the glass of milk',\n",
       " 'the glass of juice',\n",
       " 'the cup of tea',\n",
       " 'I am a good boy',\n",
       " 'I am a good developer',\n",
       " 'understand the meaning of words',\n",
       " 'your videos are good']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the vocabulary size\n",
    "voc_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the glass of milk',\n",
       " 'the glass of juice',\n",
       " 'the cup of tea',\n",
       " 'I am a good boy',\n",
       " 'I am a good developer',\n",
       " 'understand the meaning of words',\n",
       " 'your videos are good']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8924, 2573, 5464, 3358],\n",
       " [8924, 2573, 5464, 2070],\n",
       " [8924, 6241, 5464, 7311],\n",
       " [9217, 8067, 2526, 7023, 8245],\n",
       " [9217, 8067, 2526, 7023, 9842],\n",
       " [2769, 8924, 7778, 5464, 3119],\n",
       " [1361, 8829, 3574, 7023]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### One Hot Representation\n",
    "one_hot_repr = [one_hot(words,voc_size)for words in sent]   \n",
    "one_hot_repr\n",
    "\n",
    "# Converts words of each sentence into one hot encoded integers\n",
    "# The output represents the index of each word in the vocabulary. For example, the first sentence \"the glass of milk\" is represented as [6186, 6775, 637, 4895] where each number corresponds to a unique word in the vocabulary. 6186 means in a vector of size 10000, the word \"the\" is represented by the index 6186 i.e 6186th index will be 1 and all other indices will be 0.\n",
    "\n",
    "# This creates a sparse matrux which could lead to overfitting.\n",
    "# To avoid this, we can use embedding layer which will convert these sparse vectors into dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word Embedding Representation\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "#from tensorflow.keras.processing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 8924 2573 5464 3358]\n",
      " [   0    0    0    0 8924 2573 5464 2070]\n",
      " [   0    0    0    0 8924 6241 5464 7311]\n",
      " [   0    0    0 9217 8067 2526 7023 8245]\n",
      " [   0    0    0 9217 8067 2526 7023 9842]\n",
      " [   0    0    0 2769 8924 7778 5464 3119]\n",
      " [   0    0    0    0 1361 8829 3574 7023]]\n"
     ]
    }
   ],
   "source": [
    "sent_length = 8\n",
    "embedded_docs = pad_sequences(one_hot_repr, padding='pre', maxlen=sent_length)\n",
    "print(embedded_docs)\n",
    "\n",
    "# In each sentence there are a different number of words. To make them equal, we use padding. Here, we have taken the maximum length of a sentence as 8. If a sentence has less than 8 words, it will be padded with zeros at the beginning (pre-padding). If it has more than 8 words, it will be truncated to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature representation\n",
    "dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Business\\391_Simple_rnn_imdb Project\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, dim, input_length=sent_length))\n",
    "model.compile('adam', 'mse')\n",
    "\n",
    "# model.add(Embedding(voc_size, dim, input_length=sent_length)) : Adds an Embedding layer, which converts word indices (integers) into dense word vectors. \n",
    "# voc_size is the size of the vocabulary, \t\n",
    "# dim is the Embedding dimension — each word will be represented by a vector of length dim (e.g., 10 here).\n",
    "# input_length is The length of each padded sequence (e.g., 8 here) — needed so the model knows the shape of its input.\n",
    "\n",
    "# model.compile('adam', 'mse') : Compiles the model with the Adam optimizer and Mean Squared Error loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 8, 10)             100000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100000 (390.62 KB)\n",
      "Trainable params: 100000 (390.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.02685023, -0.02638841,  0.04074228,  0.00441874,\n",
       "         -0.0208487 ,  0.02411045, -0.0335707 ,  0.0363767 ,\n",
       "          0.02465141, -0.01287327],\n",
       "        [-0.00339707,  0.03555569, -0.04781895,  0.01279496,\n",
       "          0.01662549,  0.02712948,  0.02542276,  0.00889934,\n",
       "          0.04381249,  0.03135285],\n",
       "        [-0.03874917,  0.01256781, -0.0172983 , -0.00219723,\n",
       "         -0.01231436,  0.03854829, -0.0287691 , -0.02901472,\n",
       "          0.00977241, -0.02682821],\n",
       "        [-0.02280046, -0.02619243, -0.01772781, -0.00871892,\n",
       "          0.03703013, -0.00993561,  0.04030569, -0.03769493,\n",
       "         -0.01846591,  0.01010768]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.02685023, -0.02638841,  0.04074228,  0.00441874,\n",
       "         -0.0208487 ,  0.02411045, -0.0335707 ,  0.0363767 ,\n",
       "          0.02465141, -0.01287327],\n",
       "        [-0.00339707,  0.03555569, -0.04781895,  0.01279496,\n",
       "          0.01662549,  0.02712948,  0.02542276,  0.00889934,\n",
       "          0.04381249,  0.03135285],\n",
       "        [-0.03874917,  0.01256781, -0.0172983 , -0.00219723,\n",
       "         -0.01231436,  0.03854829, -0.0287691 , -0.02901472,\n",
       "          0.00977241, -0.02682821],\n",
       "        [-0.0375044 ,  0.03798864,  0.00769476, -0.00083867,\n",
       "          0.04666909,  0.0484609 ,  0.013048  , -0.00112832,\n",
       "          0.00704045, -0.01186514]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.02685023, -0.02638841,  0.04074228,  0.00441874,\n",
       "         -0.0208487 ,  0.02411045, -0.0335707 ,  0.0363767 ,\n",
       "          0.02465141, -0.01287327],\n",
       "        [-0.01596761,  0.0464415 , -0.01064597, -0.01856146,\n",
       "         -0.04330915,  0.01549828,  0.01854989, -0.03707359,\n",
       "         -0.02347124,  0.04669732],\n",
       "        [-0.03874917,  0.01256781, -0.0172983 , -0.00219723,\n",
       "         -0.01231436,  0.03854829, -0.0287691 , -0.02901472,\n",
       "          0.00977241, -0.02682821],\n",
       "        [ 0.04761774, -0.017698  , -0.02523371,  0.03813576,\n",
       "         -0.03444201, -0.00860734, -0.04400809, -0.00193604,\n",
       "          0.01741396, -0.00217045]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.00411905,  0.01613268,  0.03243111,  0.00781897,\n",
       "          0.04795029, -0.02517277,  0.02383374,  0.04863013,\n",
       "         -0.00205743,  0.01442   ],\n",
       "        [-0.00531764, -0.0072962 ,  0.04516835,  0.03926781,\n",
       "         -0.04947597, -0.00772597, -0.01272871,  0.00941715,\n",
       "          0.02063588,  0.01803331],\n",
       "        [-0.02558869,  0.01601256, -0.03464569, -0.00887769,\n",
       "          0.04616852, -0.04401643, -0.00491216, -0.01044305,\n",
       "          0.03336246, -0.0292419 ],\n",
       "        [-0.01366316, -0.00255263,  0.02577907,  0.00824832,\n",
       "         -0.01490263,  0.03313769, -0.00069825,  0.02656926,\n",
       "         -0.00119735, -0.00568167],\n",
       "        [ 0.03975167, -0.03531969, -0.01365708,  0.00681571,\n",
       "         -0.02568432, -0.01850317,  0.03938815, -0.02565945,\n",
       "         -0.00352523, -0.01830611]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.00411905,  0.01613268,  0.03243111,  0.00781897,\n",
       "          0.04795029, -0.02517277,  0.02383374,  0.04863013,\n",
       "         -0.00205743,  0.01442   ],\n",
       "        [-0.00531764, -0.0072962 ,  0.04516835,  0.03926781,\n",
       "         -0.04947597, -0.00772597, -0.01272871,  0.00941715,\n",
       "          0.02063588,  0.01803331],\n",
       "        [-0.02558869,  0.01601256, -0.03464569, -0.00887769,\n",
       "          0.04616852, -0.04401643, -0.00491216, -0.01044305,\n",
       "          0.03336246, -0.0292419 ],\n",
       "        [-0.01366316, -0.00255263,  0.02577907,  0.00824832,\n",
       "         -0.01490263,  0.03313769, -0.00069825,  0.02656926,\n",
       "         -0.00119735, -0.00568167],\n",
       "        [-0.02161977, -0.04640898,  0.01822248, -0.02273047,\n",
       "          0.01827302, -0.00406908,  0.04724171, -0.01980169,\n",
       "          0.02242077,  0.02733512]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.04585361, -0.01022978,  0.0049207 ,  0.0200314 ,\n",
       "          0.00237406,  0.02518835,  0.01958283,  0.04655052,\n",
       "          0.02028656,  0.02677004],\n",
       "        [ 0.02685023, -0.02638841,  0.04074228,  0.00441874,\n",
       "         -0.0208487 ,  0.02411045, -0.0335707 ,  0.0363767 ,\n",
       "          0.02465141, -0.01287327],\n",
       "        [-0.02420172, -0.04053061, -0.002677  ,  0.03359747,\n",
       "         -0.04398454, -0.04001036, -0.0203577 ,  0.03908609,\n",
       "         -0.0013733 , -0.01717563],\n",
       "        [-0.03874917,  0.01256781, -0.0172983 , -0.00219723,\n",
       "         -0.01231436,  0.03854829, -0.0287691 , -0.02901472,\n",
       "          0.00977241, -0.02682821],\n",
       "        [ 0.01172179,  0.02750901, -0.02348145,  0.01891506,\n",
       "         -0.0190771 ,  0.04808668,  0.03423167,  0.00662493,\n",
       "          0.01764194, -0.00186317]],\n",
       "\n",
       "       [[-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [-0.01715923,  0.01986555,  0.00510715, -0.01948509,\n",
       "          0.00451503,  0.03158509, -0.01334063, -0.00300102,\n",
       "          0.03776613, -0.00146614],\n",
       "        [ 0.00978438,  0.03594956, -0.03020099,  0.04101257,\n",
       "          0.0464008 , -0.04072879, -0.03065988,  0.02280355,\n",
       "         -0.0352755 , -0.03360294],\n",
       "        [-0.03223559, -0.01439948, -0.0097973 , -0.04891555,\n",
       "         -0.04509039,  0.00264988,  0.00417501,  0.00889917,\n",
       "         -0.02811645, -0.02765185],\n",
       "        [-0.02477188,  0.02998353,  0.00048821, -0.03880512,\n",
       "         -0.03600905, -0.04457945,  0.00660308,  0.02277889,\n",
       "          0.00376051, -0.03235918],\n",
       "        [-0.01366316, -0.00255263,  0.02577907,  0.00824832,\n",
       "         -0.01490263,  0.03313769, -0.00069825,  0.02656926,\n",
       "         -0.00119735, -0.00568167]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(embedded_docs)\n",
    "\n",
    "# This gives the dense vector representation of each word in the sentences. Each word is now represented by a vector of size 10 (as specified by `dim`), which captures the semantic meaning of the words in a more compact form compared to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 8924, 2573, 5464, 3358])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]\n",
    "\n",
    "# This is the padded output of first sentence \" the glass of milk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01453609,  0.03697893,  0.02267558,  0.01149287, -0.03695335,\n",
       "         0.01416664,  0.03655917,  0.00734384,  0.03028754,  0.00339943],\n",
       "       [ 0.01453609,  0.03697893,  0.02267558,  0.01149287, -0.03695335,\n",
       "         0.01416664,  0.03655917,  0.00734384,  0.03028754,  0.00339943],\n",
       "       [ 0.01453609,  0.03697893,  0.02267558,  0.01149287, -0.03695335,\n",
       "         0.01416664,  0.03655917,  0.00734384,  0.03028754,  0.00339943],\n",
       "       [ 0.01453609,  0.03697893,  0.02267558,  0.01149287, -0.03695335,\n",
       "         0.01416664,  0.03655917,  0.00734384,  0.03028754,  0.00339943],\n",
       "       [-0.03792738,  0.01958679, -0.04232483, -0.03475742,  0.02182527,\n",
       "         0.01143194, -0.03125288,  0.02584182,  0.0050171 ,  0.04725457],\n",
       "       [-0.02213118,  0.00730393,  0.02797868, -0.02386508, -0.0024281 ,\n",
       "         0.04419583, -0.02011771, -0.00502002, -0.03373672, -0.04126013],\n",
       "       [-0.02629154,  0.02487988, -0.02824695,  0.0302802 , -0.01835672,\n",
       "        -0.00683415,  0.01606056, -0.04426531, -0.03801771, -0.04581957],\n",
       "       [-0.01679759,  0.04462079,  0.02781102, -0.04639838,  0.0059711 ,\n",
       "         0.00940369, -0.01027383,  0.01697389, -0.01460341,  0.02758609]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(embedded_docs[0])\n",
    "\n",
    "# This gives the dense vector representation of the first sentence. Each word in the sentence is now represented by a vector of size 10, which captures the semantic meaning of the words in a more compact form compared to one-hot encoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
